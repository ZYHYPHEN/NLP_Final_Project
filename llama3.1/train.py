import matplotlib
# è®¾ç½®Aggåç«¯ï¼ˆæ— GUIç¯å¢ƒï¼‰
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import os
import json
import numpy as np
import pandas as pd
import torch
from datasets import Dataset
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer
from peft import LoraConfig, TaskType, get_peft_model

# ==================== å­—ä½“è®¾ç½® ====================
def setup_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œè§£å†³è­¦å‘Šé—®é¢˜"""
    try:
        # å°è¯•ä½¿ç”¨ç³»ç»Ÿå­—ä½“
        system_fonts = fm.findSystemFonts()
        
        # å¸¸è§ä¸­æ–‡å­—ä½“åç§°
        chinese_font_candidates = [
            'WenQuanYi Micro Hei',
            'WenQuanYi Zen Hei',
            'Noto Sans CJK',
            'Droid Sans Fallback',
            'DejaVu Sans',
            'Arial Unicode MS',
            'Microsoft YaHei',
            'SimHei',
            'SimSun'
        ]
        
        # æŸ¥æ‰¾å¯ç”¨çš„ä¸­æ–‡å­—ä½“
        available_fonts = []
        for font_path in system_fonts:
            try:
                font_prop = fm.FontProperties(fname=font_path)
                font_name = font_prop.get_name()
                if any(candidate in font_name for candidate in chinese_font_candidates):
                    available_fonts.append((font_name, font_path))
            except:
                continue
        
        if available_fonts:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„ä¸­æ–‡å­—ä½“
            font_name, font_path = available_fonts[0]
            # æ·»åŠ åˆ°matplotlib
            fm.fontManager.addfont(font_path)
            matplotlib.rcParams['font.sans-serif'] = [font_name]
            matplotlib.rcParams['axes.unicode_minus'] = False
            print(f"âœ… ä½¿ç”¨å­—ä½“: {font_name}")
        else:
            # å¦‚æœæ‰¾ä¸åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“
            print("âš ï¸  æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œå°†ä½¿ç”¨é»˜è®¤å­—ä½“")
            # è®¾ç½®å­—ä½“ä¸ºé»˜è®¤è‹±æ–‡å­—ä½“ï¼Œé¿å…ä¸­æ–‡è­¦å‘Š
            matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Helvetica']
            matplotlib.rcParams['axes.unicode_minus'] = False
            
    except Exception as e:
        print(f"âš ï¸  å­—ä½“è®¾ç½®å¤±è´¥: {e}")
        # è®¾ç½®å›é€€æ–¹æ¡ˆ
        matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Helvetica']
        matplotlib.rcParams['axes.unicode_minus'] = False

# åˆå§‹åŒ–å­—ä½“è®¾ç½®
setup_chinese_font()

# ==================== æ•°æ®å¤„ç†å‡½æ•° ====================
def process_func(example):
    MAX_LENGTH = 384
    input_ids, attention_mask, labels = [], [], []
    instruction = tokenizer(
        f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 15 Dec 2025\n\nç°åœ¨ä½ è¦æ‰®æ¼”ç¥è¯è§’è‰²â€”â€”å­™æ‚Ÿç©º<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{example['instruction'] + example['input']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
        add_special_tokens=False
    )
    response = tokenizer(f"{example['output']}<|eot_id|>", add_special_tokens=False)
    
    input_ids = instruction["input_ids"] + response["input_ids"] + [tokenizer.pad_token_id]
    attention_mask = instruction["attention_mask"] + response["attention_mask"] + [1]
    labels = [-100] * len(instruction["input_ids"]) + response["input_ids"] + [tokenizer.pad_token_id]
    
    if len(input_ids) > MAX_LENGTH:
        input_ids = input_ids[:MAX_LENGTH]
        attention_mask = attention_mask[:MAX_LENGTH]
        labels = labels[:MAX_LENGTH]
    
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }

# ==================== è‡ªå®šä¹‰Trainer ====================
class CustomTrainer(Trainer):
    """è‡ªå®šä¹‰Trainerä»¥è®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.train_losses = []
        self.eval_losses = []
        self.train_steps = []
        self.eval_steps = []
        self.best_eval_loss = float('inf')
        
    def compute_loss(self, model, inputs, return_outputs=False):
        # è®¡ç®—æŸå¤±
        outputs = model(**inputs)
        loss = outputs.loss if isinstance(outputs, dict) else outputs[0]
        
        # è®°å½•è®­ç»ƒæŸå¤±ï¼ˆåªåœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼‰
        if model.training:
            self.train_losses.append(loss.item())
            self.train_steps.append(self.state.global_step)
            
        return (loss, outputs) if return_outputs else loss
    
    def evaluation_loop(self, *args, **kwargs):
        # æ‰§è¡Œè¯„ä¼°å¹¶è®°å½•è¯„ä¼°æŸå¤±
        output = super().evaluation_loop(*args, **kwargs)
        
        if output.metrics.get("eval_loss") is not None:
            eval_loss = output.metrics["eval_loss"]
            self.eval_losses.append(eval_loss)
            self.eval_steps.append(self.state.global_step)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if eval_loss < self.best_eval_loss:
                self.best_eval_loss = eval_loss
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                best_model_path = os.path.join(self.args.output_dir, "best_model")
                self.save_model(best_model_path)
                print(f"\nâœ¨ æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼ŒéªŒè¯æŸå¤±: {eval_loss:.4f}")
                
        return output
    
    # def plot_loss_curves(self, output_dir):
    #     """ç»˜åˆ¶æŸå¤±æ›²çº¿å¹¶ä¿å­˜"""
    #     plt.figure(figsize=(12, 6))
        
    #     # ç»˜åˆ¶è®­ç»ƒæŸå¤±
    #     if self.train_losses:
    #         # å¹³æ»‘è®­ç»ƒæŸå¤±æ›²çº¿
    #         smooth_loss = np.convolve(self.train_losses, np.ones(10)/10, mode='valid')
    #         smooth_steps = self.train_steps[:len(smooth_loss)]
    #         plt.plot(smooth_steps, smooth_loss, 'b-', label='è®­ç»ƒæŸå¤±', alpha=0.7, linewidth=1.5)
            
    #     # ç»˜åˆ¶éªŒè¯æŸå¤±
    #     if self.eval_losses and self.eval_steps:
    #         plt.plot(self.eval_steps, self.eval_losses, 'r-', label='éªŒè¯æŸå¤±', alpha=0.7, linewidth=2, marker='o')
            
    #     plt.xlabel('è®­ç»ƒæ­¥æ•° (Steps)', fontsize=12)
    #     plt.ylabel('æŸå¤± (Loss)', fontsize=12)
    #     plt.title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿', fontsize=14, fontweight='bold')
    #     plt.legend(fontsize=11)
    #     plt.grid(True, alpha=0.3)
        
    #     # ä¿å­˜æŸå¤±æ›²çº¿å›¾
    #     loss_plot_path = os.path.join(output_dir, "loss_curves.png")
    #     plt.tight_layout()
    #     plt.savefig(loss_plot_path, dpi=300, bbox_inches='tight')
    #     plt.close()
        
    #     print(f"ğŸ“ˆ æŸå¤±æ›²çº¿å·²ä¿å­˜è‡³: {loss_plot_path}")
        
    #     # ä¿å­˜æŸå¤±æ•°æ®ä¸ºJSONæ–‡ä»¶
    #     loss_data = {
    #         "train_losses": self.train_losses,
    #         "train_steps": self.train_steps,
    #         "eval_losses": self.eval_losses,
    #         "eval_steps": self.eval_steps,
    #         "best_eval_loss": self.best_eval_loss
    #     }
        
    #     loss_data_path = os.path.join(output_dir, "loss_data.json")
    #     with open(loss_data_path, 'w', encoding='utf-8') as f:
    #         json.dump(loss_data, f, ensure_ascii=False, indent=2)
        
    #     print(f"ğŸ“Š æŸå¤±æ•°æ®å·²ä¿å­˜è‡³: {loss_data_path}")
        
    #     return loss_data

    def plot_loss_curves(self, output_dir):
        """ç»˜åˆ¶æŸå¤±æ›²çº¿å¹¶ä¿å­˜"""
        plt.figure(figsize=(12, 6))
        
        # ä½¿ç”¨è‹±æ–‡æ ‡ç­¾
        if self.train_losses:
            plt.plot(self.train_steps, self.train_losses, 'b-', label='Training Loss', alpha=0.7, linewidth=2)
            
        if self.eval_losses and self.eval_steps:
            plt.plot(self.eval_steps, self.eval_losses, 'r-', label='Validation Loss', alpha=0.7, linewidth=2, marker='o')
            
        plt.xlabel('Training Steps', fontsize=12)
        plt.ylabel('Loss', fontsize=12)
        plt.title('Training and Validation Loss Curves', fontsize=14, fontweight='bold')
        plt.legend(fontsize=11)
        plt.grid(True, alpha=0.3)
        
        # ä¿å­˜æŸå¤±æ›²çº¿å›¾
        loss_plot_path = os.path.join(output_dir, "loss_curves.png")
        plt.tight_layout()
        plt.savefig(loss_plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“ˆ Loss curves saved to: {loss_plot_path}")
        
        # ä¿å­˜æŸå¤±æ•°æ®ä¸ºJSONæ–‡ä»¶
        loss_data = {
            "train_losses": self.train_losses,
            "train_steps": self.train_steps,
            "eval_losses": self.eval_losses,
            "eval_steps": self.eval_steps,
            "best_eval_loss": self.best_eval_loss
        }
        
        loss_data_path = os.path.join(output_dir, "loss_data.json")
        with open(loss_data_path, 'w', encoding='utf-8') as f:
            json.dump(loss_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š Loss data saved to: {loss_data_path}")
        
        return loss_data

# ==================== ä¸»ç¨‹åº ====================
if __name__ == "__main__":
    # 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    model = AutoModelForCausalLM.from_pretrained(
        './LLM-Research/Meta-Llama-3___1-8B', 
        device_map="auto",
        torch_dtype=torch.bfloat16
    )
    model.enable_input_require_grads()
    
    tokenizer = AutoTokenizer.from_pretrained(
        './LLM-Research/Meta-Llama-3___1-8B', 
        use_fast=False, 
        trust_remote_code=True
    )
    tokenizer.pad_token = tokenizer.eos_token
    
    # 2. åŠ è½½å¹¶å¤„ç†æ•°æ®
    df = pd.read_json('chat_wukong1.json')
    
    # 3. åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›† (80%è®­ç»ƒ, 20%éªŒè¯)
    train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42)
    
    print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    print(f"  è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_df)}")
    print(f"  éªŒè¯é›†æ ·æœ¬æ•°: {len(eval_df)}")
    print(f"  æ€»æ ·æœ¬æ•°: {len(df)}")
    
    # åˆ›å»ºè®­ç»ƒé›†å’ŒéªŒè¯é›†Dataset
    train_ds = Dataset.from_pandas(train_df)
    eval_ds = Dataset.from_pandas(eval_df)
    
    # å¤„ç†æ•°æ®
    tokenized_train = train_ds.map(process_func, remove_columns=train_ds.column_names)
    tokenized_eval = eval_ds.map(process_func, remove_columns=eval_ds.column_names)
    
    # 4. é…ç½®LoRA
    config = LoraConfig(
        task_type=TaskType.CAUSAL_LM, 
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        inference_mode=False,
        r=8,
        lora_alpha=32,
        lora_dropout=0.1
    )
    model = get_peft_model(model, config)
    model.print_trainable_parameters()
    
    # 5. é…ç½®è®­ç»ƒå‚æ•°
    output_dir = "./output/llama3_1_lora"
    
    args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=4,
        per_device_eval_batch_size=4,
        gradient_accumulation_steps=4,
        logging_steps=10,
        eval_steps=50,
        save_steps=100,
        num_train_epochs=3,
        learning_rate=1e-4,
        save_on_each_node=True,
        gradient_checkpointing=True,
        evaluation_strategy="steps",
        save_strategy="steps",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        report_to="none",
        logging_dir=f"{output_dir}/logs",
        save_total_limit=3,
        dataloader_num_workers=4,
        fp16=False,
        bf16=True,
        remove_unused_columns=False,
    )
    
    # 6. åˆ›å»ºæ•°æ®æ”¶é›†å™¨
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer, 
        padding=True,
        pad_to_multiple_of=8
    )
    
    # 7. åˆ›å»ºè‡ªå®šä¹‰Trainer
    trainer = CustomTrainer(
        model=model,
        args=args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_eval,
        data_collator=data_collator,
    )
    
    # 8. å¼€å§‹è®­ç»ƒ
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    train_result = trainer.train()
    
    # 9. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    trainer.save_model()
    trainer.save_state()
    
    # 10. ç»˜åˆ¶æŸå¤±æ›²çº¿
    loss_data = trainer.plot_loss_curves(output_dir)
    
    # 11. æ‰“å°è®­ç»ƒæ‘˜è¦
    print("\n" + "="*50)
    print("ğŸ è®­ç»ƒå®Œæˆ!")
    print("="*50)
    print(f"æœ€ä½³éªŒè¯æŸå¤±: {trainer.best_eval_loss:.4f}")
    print(f"æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_result.training_loss:.4f}")
    print(f"æ¨¡å‹ä¿å­˜åœ¨: {output_dir}")
    print(f"æœ€ä½³æ¨¡å‹ä¿å­˜åœ¨: {os.path.join(output_dir, 'best_model')}")
    print("="*50)
    
    # 12. å¯é€‰ï¼šåœ¨éªŒè¯é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°
    print("\nğŸ“‹ åœ¨éªŒè¯é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
    eval_results = trainer.evaluate()
    print(f"æœ€ç»ˆéªŒè¯æŸå¤±: {eval_results['eval_loss']:.4f}")